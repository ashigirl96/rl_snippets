"""One file REINFORCE algorithm."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import itertools

import collections
import gym
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from agents.tools.attr_dict import AttrDict
from agents.tools.wrappers import ConvertTo32Bit
from ray.experimental.tfutils import TensorFlowVariables
from ray.rllib.utils.filter import MeanStdFilter

from reinforce.utils import bcolors

Transition = collections.namedtuple('Transition',
                                    'observ, reward, done, action, next_observ, raw_return, return_')
Transition.__new__.__defaults__ = (None,) * len(Transition._fields)


class Policy(object):
    
    def __init__(self, sess: tf.Session, config):
        """Neural network policy that compute action given observation.
        
        Args:
            config: Useful configuration, almost use as const.
        """
        self.sess = sess
        
        self.config = config
        self._build_model()
        self._set_loss()
        optimizer = tf.train.AdamOptimizer(config.learning_rate)
        # grads_and_vars = optimizer.compute_gradients(self.loss)
        # self.train_op = optimizer.apply_gradients(grads_and_vars)
        self.train_op = optimizer.minimize(self.loss)
        
        self.variables = TensorFlowVariables(self.loss, self.sess)
        self.sess.run(tf.global_variables_initializer())
    
    def _build_model(self):
        """Build TensorFlow policy model."""
        # To look clearly, whether I use bias.
        use_bias = self.config.use_bias
        activation = self.config.activation
        
        # Placeholders. observ = Box(4,), action = Discrete(2,)
        # I have to describe which action do I select.
        self.observ = tf.placeholder(tf.float32, (None, 4), name='observ')
        self.action = tf.placeholder(tf.int32, name='action')
        self.expected_value = tf.placeholder(tf.float32, name='expected_value')
        x = tf.layers.dense(self.observ, 2, use_bias=use_bias,
                            kernel_initializer=tf.zeros_initializer,
                            bias_initializer=tf.ones_initializer)
        self.logits = x
        self.action_probs = tf.squeeze(tf.nn.softmax(self.logits))
        self.picked_action_prob = tf.gather(self.action_probs, self.action)
    
    def _set_loss(self):
        # For use `tf.nn.sparse_softmax_cross_entropy_with_logits`,
        # The shape of action should be `(1, ...)`
        log_prob = -tf.log(self.picked_action_prob) * self.expected_value
        log_prob = tf.check_numerics(log_prob, 'log_prob')
        self.loss = log_prob
    
    def compute_action(self, observ):
        """Generate action from \pi(a_t | s_t) that is neural network.
        
        Args:
            observ: Observation generated by gym.Env.observation.

        Returns:
            (Lights, Camera) Action
        """
        assert observ.shape == (1, 4)
        action_probs = self.sess.run(self.action_probs, feed_dict={self.observ: observ})
        action_probs = np.squeeze(action_probs)
        
        # Note selection disappeared only discrete action model.
        action = np.random.choice(np.arange(len(action_probs)), p=action_probs).astype(np.int64)
        assert isinstance(action, np.int64)
        return action


def eps_greedy(action_probs, eps):
    argmax = np.argmax(action_probs)
    action_probs = np.ones_like(action_probs) * eps / len(action_probs)
    action_probs[argmax] += 1 - eps
    return action_probs


def greedy(action_probs):
    action = int(np.argmax(action_probs))
    return action


# TODO: I WILL IMPLEMENT.
class ValueFunction(object):
    
    def __init__(self, sess: tf.Session, config):
        """Neural network policy that compute action given observation.
        
        Args:
            config: Useful configuration, almost use as const.
        """
        self.sess = sess
        
        self.config = config
        self._build_model()
        self._set_loss()
        optimizer = tf.train.AdamOptimizer(config.learning_rate)
        self.train_op = optimizer.minimize(self.loss)
        
        self.variables = TensorFlowVariables(self.loss, self.sess)
        self.sess.run(tf.global_variables_initializer())
    
    def _build_model(self):
        # To look clearly, whether I use bias.
        use_bias = self.config.use_bias
        activation = self.config.activation
        
        self.observ = tf.placeholder(tf.float32, (None, 4), name='observ')
        self.return_ = tf.placeholder(tf.float32, name='return_')
        x = tf.layers.dense(self.observ, 1, use_bias=use_bias,
                            kernel_initializer=tf.zeros_initializer,
                            bias_initializer=tf.zeros_initializer)
        self.logits = x
    
    def _set_loss(self):
        losses = tf.losses.mean_squared_error(labels=self.return_,
                                              predictions=self.logits)
        self.loss = tf.reduce_mean(losses)


class REINFORCE(object):
    
    def __init__(self, config):
        tf.reset_default_graph()
        self.sess = tf.Session()
        
        env = gym.make(config.env_name)
        self.config = config
        self.env = ConvertTo32Bit(env)
        self.policy = Policy(sess=self.sess, config=config)
        self.value_func = ValueFunction(sess=self.sess, config=config)
        
        self._init()
    
    def _init(self):
        self.reward_filter = MeanStdFilter((), clip=5.)
    
    def compute_trajectory(self):
        trajectory = rollouts(self.env,
                              self.policy,
                              self.reward_filter,
                              self.config)
        return trajectory
    
    def _train(self):
        """REINFORCE Algorithm.
        
        Returns: Losses each timestep.
        """
        trajectories = []
        for _ in range(self.config.num_episodes):
            trajectory = self.compute_trajectory()
            best_return = trajectory[-1].raw_return
            message = "Best Return: {0}".format(best_return)
            print('{0}{1}{2}'.format(bcolors.OKBLUE, message, bcolors.ENDC))
            trajectories.append(trajectory)
        
        losses = []
        for trajectory in trajectories:
            for transition in trajectory:
                _, loss, expected_return = self.sess.run(
                    [self.value_func.train_op, self.value_func.loss, self.value_func.logits],
                    feed_dict={
                        self.value_func.observ: transition.observ,
                        self.value_func.return_: transition.return_,
                    })
                advantage = transition.return_ - expected_return
                _, loss = self.sess.run(
                    [self.policy.train_op, self.policy.loss], feed_dict={
                        self.policy.observ: transition.observ,
                        self.policy.action: transition.action,
                        # self.policy.expected_value: transition.return_,
                        self.policy.expected_value: advantage,
                    })
                losses.append(loss)
        return losses
    
    def train(self, num_iters):
        saver = tf.train.Saver()
        for i in range(num_iters):
            losses = self._train()
            yield losses
        saver.save(self.sess, './reinforce_debug')


def rollouts(env, policy: Policy, reward_filter: MeanStdFilter, config):
    """
    Args:
        env: OpenAI Gym wrapped by agents.wrappers
        policy(Policy): instance of Policy
        reward_filter(MeanStdFilter): Use ray's MeanStdFilter for calculate easier
        config: Useful configuration, almost use as const.

    Returns:
        1 episode(rollout) that is sequence of trajectory.
    """
    raw_return = 0
    return_ = 0
    observ = env.reset()
    observ = observ[np.newaxis, ...]
    
    trajectory = []
    for t in itertools.count():
        # a_t ~ pi(a_t | s_t)
        action = policy.compute_action(observ)
        
        next_observ, reward, done, _ = env.step(action)
        
        # This rollout does not provide batch observ and action.
        next_observ = next_observ[np.newaxis, ...]
        
        # Adjust reward
        # reward = reward_filter(reward)
        raw_return += reward
        return_ += reward * config.discount_factor ** t
        
        # Make trajectory sample.
        trajectory.append(Transition(observ, reward, done, action, next_observ, raw_return, return_))
        
        # s_{t+1} ‚Üê s_{t}
        observ = next_observ
        
        if done:
            break
    return trajectory


def evaluate_policy(policy, config):
    """
    Args:
        policy(Policy): instance of Policy
    Returns:
        score
    """
    env = gym.make(config.env_name)
    
    raw_return = 0
    observ = env.reset()
    observ = observ[np.newaxis, ...]
    
    for t in itertools.count():
        # a_t ~ pi(a_t | s_t)
        action = policy.compute_action(observ)
        observ, reward, done, _ = env.step(action)
        observ = observ[np.newaxis, ...]
        raw_return += reward
        
        if done:
            break
    return raw_return


def default_config():
    # Whether use bias on layer
    use_bias = True
    # OpenAI Gym environment name
    env_name = 'CartPole-v0'
    # Discount Factor (gamma)
    discount_factor = 1.
    # Learning rate
    learning_rate = 1e-4
    # Number of episodes
    num_episodes = 1
    # Activation function used in dense layer
    activation = tf.nn.relu
    # Epsilon-Greedy Policy
    eps = 0.1
    
    return locals()


def main(_):
    config = AttrDict(default_config())
    # Define Agent that train with REINFORCE algorithm.
    agent = REINFORCE(config)
    
    # Train for num_iters times.
    episode_loss = []
    episode_score = []
    for i, losses in enumerate(agent.train(num_iters=1000)):
        loss = np.mean(losses)
        # Evaluate the policy so that it will mean score.
        score = np.mean([evaluate_policy(agent.policy, config) for _ in range(5)])
        message = 'episode: {0}, loss: {1}, score: {2}'.format(i, loss, score)
        print('{0}{1}{2}'.format(bcolors.HEADER, message, bcolors.ENDC))
        episode_loss.append(loss)
        episode_score.append(score)
        x = np.arange(len(episode_loss))
    fig, (axL, axR) = plt.subplots(nrows=2, figsize=(10, 10))
    axL.plot(x, episode_loss)
    axL.legend(['loss'])
    axR.plot(x, episode_score, color='red')
    axR.legend(['score'])
    plt.show()


if __name__ == '__main__':
    tf.set_random_seed(42)
    np.random.seed(42)
    tf.app.run()